import numpy as np
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.model_selection import StratifiedKFold, cross_validate
from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score
from sklearn.feature_extraction.text import TfidfVectorizer
from scipy.sparse import hstack
import joblib
import argparse
import sys
import os

# Import c·∫•u h√¨nh v√† h√†m ti·ªán √≠ch t·ª´ c√°c file b·∫°n ƒë√£ t·∫°o tr∆∞·ªõc ƒë√≥
from config import RANDOM_STATE, CV_FOLDS, TFIDF_MAX_FEATURES, DEFAULT_BACKEND, MODEL_PATH, VECTORIZER_PATH, ENCODERS_PATH, DATA_PATH
from utils import logger, save_artifacts, ensure_binary_labels
from preprocess import read_csv_safe, auto_label, feature_engineer

# S·ª≠a l·ªói hi·ªÉn th·ªã ti·∫øng Vi·ªát tr√™n Windows console
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8')

def get_model(backend='xgboost'):
    """
    Factory ƒë·ªÉ t·∫°o model d·ª±a tr√™n backend ƒë∆∞·ª£c ch·ªçn.
    H·ªó tr·ª£ XGBoost, LightGBM, CatBoost.
    """
    if backend == 'xgboost':
        from xgboost import XGBClassifier
        return XGBClassifier(
            n_estimators=300,
            max_depth=6,
            learning_rate=0.1,
            use_label_encoder=False,
            eval_metric='logloss',
            n_jobs=-1,
            random_state=RANDOM_STATE
        )
    elif backend == 'lightgbm':
        import lightgbm as lgb
        return lgb.LGBMClassifier(n_estimators=1000, random_state=RANDOM_STATE, n_jobs=-1)
    elif backend == 'catboost':
        from catboost import CatBoostClassifier
        return CatBoostClassifier(iterations=500, verbose=100, random_state=RANDOM_STATE)
    else:
        raise ValueError(f"Backend '{backend}' ch∆∞a ƒë∆∞·ª£c h·ªó tr·ª£ ho·∫∑c ch∆∞a c√†i ƒë·∫∑t.")

def train_pipeline(backend=DEFAULT_BACKEND):
    logger.info(f"üöÄ Starting training pipeline with backend: {backend}")
    
    # --- 1. Load & Preprocess ---
    # ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV (ƒë∆∞·ªùng d·∫´n l·∫•y t·ª´ config.py)
    logger.info(f"Reading data from: {DATA_PATH}")
    df = read_csv_safe(DATA_PATH)
    
    # G√°n nh√£n t·ª± ƒë·ªông (Auto-labeling) ƒë·ªÉ c√≥ d·ªØ li·ªáu train
    df = auto_label(df)
    
    # Feature Engineering: T·∫°o ƒë·∫∑c tr∆∞ng v√† l·∫•y nh√£n y
    # is_training=True ƒë·ªÉ h√†m tr·∫£ v·ªÅ c·∫£ y
    X_num, X_cat, X_text, y = feature_engineer(df, is_training=True)
    
    # ƒê·∫£m b·∫£o nh√£n y l√† nh·ªã ph√¢n (0/1)
    y = ensure_binary_labels(y)

    # Ki·ªÉm tra s∆° b·ªô d·ªØ li·ªáu
    n_threats = sum(y)
    logger.info(f"Data shape: {len(df)} rows. Threat ratio: {n_threats}/{len(y)} ({n_threats/len(y):.2%})")

    if n_threats == 0:
        logger.warning("‚ö†Ô∏è C·∫¢NH B√ÅO: Kh√¥ng c√≥ m·∫´u Threat n√†o trong d·ªØ li·ªáu! Model s·∫Ω h·ªçc kh√¥ng hi·ªáu qu·∫£.")
        logger.warning("üí° G·ª£i √Ω: H√£y ch·∫°y t·∫•n c√¥ng gi·∫£ l·∫≠p (net user /add...) r·ªìi ch·∫°y l·∫°i fetch_alerts.py")

    # --- 2. X√¢y d·ª±ng Transformers (B·ªô bi·∫øn ƒë·ªïi d·ªØ li·ªáu) ---
    numeric_features = list(X_num.columns)
    categorical_features = list(X_cat.columns)

    # Pipeline cho d·ªØ li·ªáu s·ªë: Chu·∫©n h√≥a (StandardScaler)
    num_transformer = Pipeline(steps=[
        ('scaler', StandardScaler())
    ])
    
    # Pipeline cho d·ªØ li·ªáu danh m·ª•c: One-Hot Encoding (bi·∫øn ch·ªØ th√†nh vector 0/1)
    # handle_unknown='ignore': G·∫∑p gi√° tr·ªã l·∫° th√¨ b·ªè qua, kh√¥ng l·ªói
    cat_transformer = Pipeline(steps=[
        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=True)) 
    ])

    # B·ªô x·ª≠ l√Ω c·ªôt (ColumnTransformer) ƒë·ªÉ √°p d·ª•ng ri√™ng t·ª´ng lo·∫°i
    preprocessor = ColumnTransformer(
        transformers=[
            ('num', num_transformer, numeric_features),
            ('cat', cat_transformer, categorical_features)
        ], remainder='drop'
    )

    # NLP: TF-IDF Vectorizer cho d·ªØ li·ªáu vƒÉn b·∫£n
    vectorizer = TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, ngram_range=(1,2))

    # --- 3. Chu·∫©n b·ªã d·ªØ li·ªáu Train ---
    logger.info("‚öôÔ∏è  Transforming features...")
    
    # Bi·∫øn ƒë·ªïi s·ªë & category
    X_pre = preprocessor.fit_transform(X_num.join(X_cat))
    
    # Bi·∫øn ƒë·ªïi text
    # N·∫øu kh√¥ng c√≥ text th√¨ t·∫°o ma tr·∫≠n r·ªóng
    if X_text is not None and not X_text.empty and not (X_text == '').all():
        X_text_tfidf = vectorizer.fit_transform(X_text)
    else:
        from scipy.sparse import csr_matrix
        X_text_tfidf = csr_matrix((X_pre.shape[0], 0))
        logger.warning("‚ö†Ô∏è No text data found for TF-IDF.")
    
    # G·ªôp l·∫°i th√†nh ma tr·∫≠n l·ªõn (Sparse Matrix)
    X_full = hstack([X_pre, X_text_tfidf])

    # --- 4. Cross-Validation (Ki·ªÉm tra ch√©o) ---
    model = get_model(backend)
    
    # Chia t·∫≠p d·ªØ li·ªáu th√†nh 5 ph·∫ßn ƒë·ªÉ test ch√©o
    cv = StratifiedKFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE)
    
    # C√°c ch·ªâ s·ªë ƒë√°nh gi√°
    scoring = {
        'accuracy': make_scorer(accuracy_score),
        'f1': make_scorer(f1_score, zero_division=0)
    }

    logger.info('üîÑ Running Cross-Validation...')
    
    # Ki·ªÉm tra xem c√≥ ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ split kh√¥ng (√≠t nh·∫•t 2 class)
    if len(np.unique(y)) < 2:
        logger.warning("‚ö†Ô∏è D·ªØ li·ªáu ch·ªâ c√≥ 1 l·ªõp (to√†n an to√†n ho·∫∑c to√†n nguy hi·ªÉm). B·ªè qua Cross-Validation.")
    else:
        res = cross_validate(model, X_full, y, cv=cv, scoring=scoring, return_train_score=False, n_jobs=-1)
        for k, v in res.items():
            logger.info(f"   CV {k}: mean={np.mean(v):.4f} std={np.std(v):.4f}")

    # --- 5. Train Final Model (Tr√™n to√†n b·ªô d·ªØ li·ªáu) ---
    logger.info('üß† Fitting final model...')
    model.fit(X_full, y)

    # --- 6. L∆∞u tr·ªØ (Save Artifacts) ---
    # L∆∞u preprocessor (ch·ª©a scaler v√† onehot) ƒë·ªÉ d√πng l·∫°i khi d·ª± ƒëo√°n
    artifacts = {
        'preprocessor': preprocessor,
        'numeric_features': numeric_features,
        'categorical_features': categorical_features
    }
    
    save_artifacts(model, artifacts, vectorizer, MODEL_PATH, ENCODERS_PATH, VECTORIZER_PATH)
    logger.info('üéâ Training complete!')

if __name__ == '__main__':
    parser = argparse.ArgumentParser()
    parser.add_argument('--backend', default=DEFAULT_BACKEND, help='xgboost|lightgbm|catboost')
    args = parser.parse_args()
    
    try:
        train_pipeline(backend=args.backend)
    except Exception as e:
        logger.error(f"Training failed: {e}")
        import traceback
        traceback.print_exc()